---
layout: post
title: 《统计学习方法》—— 感知机
category: 统计学习方法
---

### 前言

感知机是最简单的线性二分类模型，正是因为太简单，反而常常被忽略。学完这一章，才发现从简单的东西里，也能学到不少深刻的原理。



### 关键知识点

##### 感知机（perceptron）是二类分类的线性分类模型
- 输入：实例的特征向量
- 输出：实例的类别，取+1和-1值
- 目标：寻找将数据点分成正、负两类的超平面

![感知机]({{site.url}}/images/figures/统计学习方法2-1.png)

##### 损失函数
- 损失函数的一个自然选择是误分类点的总数。但是，这样的损失函数不是参数 w，b 的连续可导函数，不易优化。损失函数的另一个选择是误分类点到超平面S的总距离，这是感知机所采用的。

![损失函数]({{site.url}}/images/figures/统计学习方法2-2.png)

- 通过随机梯度下降消除误分类点，直观上有如下解释：
当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整 w，b 的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。
此外，感知机学习算法由于采用不同的初值或选取不同的误分类点，解可以不同。
<br/><br/>

##### 感知机学习算法的对偶形式

![损失函数]({{site.url}}/images/figures/统计学习方法2-3.png)

- 其中的 ni 代表对 i 个样本的学习次数。
<br/><br/>

##### 算法步骤

![损失函数]({{site.url}}/images/figures/统计学习方法2-4.png)

##### Gram 矩阵

![损失函数]({{site.url}}/images/figures/统计学习方法2-5.png)



### 认知提升

#1 第一次知道原来感知机还有对偶形式。而且在一开始的时候，因为没有明确提ni的含义，造成对整个算法的不理解。最后还是因为搜到了一篇文章(见“参考材料 #1”)，看到里面的解释才恍然大悟。看来，很多看起来不好理解的复杂事情，可能就只是因为对某个点的不理解或者误解，就会造成对理解整个事物的无力感。但是一旦这个点获得突破，就会豁然开朗。这样的时刻，无疑是美妙的。

#2 之前常常看到一些课程或者代码中将输入向量添加常数1，受这一章里面内容的启发，明白了是因为为了便于推导，会将偏置并入权重成为单一的向量，因此输入向量上会添加1来保持对齐。



### TODO
#1 待深入理解<br/>
由 Novikoff 定理，表明当训练数据集线性可分时，感知机学习算法原始形式迭代是收敛的。

#2 如何证明？<br/>
当训练集线性不可分时，感知机学习算法不收敛，迭代结果会发生震荡。

#3 待深入了解<br/>
口袋算法（pocket algorithm）、表决感知机（voted perceptron）、带边缘感知机（perceptron with margin）

#4 如何证明？<br/>
定理：样本集线性可分的充分必要条件是正实例点集所构成的凸壳与负实例点集所构成的凸壳互不相交。



### 参考材料
1. 统计学习方法(2)——感知机原始形式、对偶形式及Python实现<br/>
<https://blog.csdn.net/winter_evening/article/details/70196040/>
2. Gram矩阵<br/>
<https://blog.csdn.net/wangyang20170901/article/details/79037867/>
